# AUTOGENERATED! DO NOT EDIT!

import asyncio
import pytest
import tempfile
import shutil
from pathlib import Path

from repoyard.cmds import (
    new_repo,
    sync_repo,
    exclude_repo,
    include_repo,
)
from repoyard.cmds._force_push_to_remote import force_push_to_remote
from repoyard._models import get_repoyard_meta, RepoPart, SyncRecord
from repoyard.config import get_config
from repoyard import const
from repoyard._utils.rclone import rclone_lsjson

from ...integration.conftest import create_repoyards

@pytest.mark.integration
def test_force_push_to_remote():
    """Test force-push command for pushing arbitrary folders to remote."""
    asyncio.run(_test_force_push_to_remote())

async def _test_force_push_to_remote():
    remote_name, remote_rclone_path, config, config_path, data_path = create_repoyards()
    repo_index_name = new_repo(
        config_path=config_path,
        repo_name="test-force-push-repo",
        storage_location=remote_name,
    )
    
    # Refresh config and get repo meta
    config = get_config(config_path)
    repoyard_meta = get_repoyard_meta(config, force_create=True)
    repo_meta = repoyard_meta.by_index_name[repo_index_name]
    
    # Add some initial test data
    local_data_path = repo_meta.get_local_part_path(config, RepoPart.DATA)
    initial_file = local_data_path / "initial_file.txt"
    initial_file.write_text("Initial content")
    
    # Sync to remote
    await sync_repo(config_path=config_path, repo_index_name=repo_index_name)
    
    # Verify initial file is on remote
    remote_data_path = repo_meta.get_remote_part_path(config, RepoPart.DATA)
    remote_files = await rclone_lsjson(
        config.rclone_config_path,
        source=remote_name,
        source_path=str(remote_data_path),
    )
    assert any(f["Name"] == "initial_file.txt" for f in remote_files)
    with tempfile.TemporaryDirectory() as temp_dir:
        source_path = Path(temp_dir) / "my_source"
        source_path.mkdir(parents=True)
        (source_path / "new_file.txt").write_text("New content")
    
        try:
            await force_push_to_remote(
                config_path=config_path,
                repo_index_name=repo_index_name,
                source_path=source_path,
                force=False,  # Without --force
            )
            assert False, "Should have raised ValueError"
        except ValueError as e:
            assert "destructive operation" in str(e)
            assert "--force" in str(e)
    non_existent_path = Path("/tmp/non_existent_source_12345")
    assert not non_existent_path.exists()
    
    try:
        await force_push_to_remote(
            config_path=config_path,
            repo_index_name=repo_index_name,
            source_path=non_existent_path,
            force=True,
        )
        assert False, "Should have raised ValueError"
    except ValueError as e:
        assert "does not exist" in str(e)
    with tempfile.TemporaryDirectory() as temp_dir:
        source_path = Path(temp_dir) / "my_source"
        source_path.mkdir(parents=True)
        (source_path / "new_file.txt").write_text("New content from force push")
        (source_path / "another_file.txt").write_text("Another new file")
    
        await force_push_to_remote(
            config_path=config_path,
            repo_index_name=repo_index_name,
            source_path=source_path,
            force=True,
            verbose=True,
        )
    
        # Verify new files are on remote
        remote_files = await rclone_lsjson(
            config.rclone_config_path,
            source=remote_name,
            source_path=str(remote_data_path),
        )
        file_names = [f["Name"] for f in remote_files]
        assert "new_file.txt" in file_names
        assert "another_file.txt" in file_names
        # Initial file should be gone (sync replaces everything)
        assert "initial_file.txt" not in file_names
    # Read sync records and verify they're complete
    local_sync_record_path = repo_meta.get_local_sync_record_path(config, RepoPart.DATA)
    assert local_sync_record_path.exists()
    
    local_record = SyncRecord.model_validate_json(local_sync_record_path.read_text())
    assert local_record.sync_complete == True
    
    # Read remote sync record
    remote_sync_record = await SyncRecord.rclone_read(
        config.rclone_config_path,
        remote_name,
        str(repo_meta.get_remote_sync_record_path(config, RepoPart.DATA)),
    )
    assert remote_sync_record is not None
    assert remote_sync_record.sync_complete == True
    
    # Verify ULIDs match
    assert local_record.ulid == remote_sync_record.ulid
    # First, exclude the repo to remove local DATA
    await exclude_repo(config_path=config_path, repo_index_name=repo_index_name)
    
    # Verify it's excluded
    config = get_config(config_path)
    repoyard_meta = get_repoyard_meta(config, force_create=True)
    repo_meta = repoyard_meta.by_index_name[repo_index_name]
    assert not repo_meta.check_included(config)
    
    # Now force push from an arbitrary source to the excluded repo
    with tempfile.TemporaryDirectory() as temp_dir:
        source_path = Path(temp_dir) / "restore_source"
        source_path.mkdir(parents=True)
        (source_path / "restored_file.txt").write_text("Restored content")
    
        await force_push_to_remote(
            config_path=config_path,
            repo_index_name=repo_index_name,
            source_path=source_path,
            force=True,
            verbose=True,
        )
    
        # Verify the file is on remote
        remote_files = await rclone_lsjson(
            config.rclone_config_path,
            source=remote_name,
            source_path=str(remote_data_path),
        )
        file_names = [f["Name"] for f in remote_files]
        assert "restored_file.txt" in file_names
    # Include the repo again
    await include_repo(config_path=config_path, repo_index_name=repo_index_name)
    
    # Refresh config and verify the content
    config = get_config(config_path)
    repoyard_meta = get_repoyard_meta(config, force_create=True)
    repo_meta = repoyard_meta.by_index_name[repo_index_name]
    
    local_data_path = repo_meta.get_local_part_path(config, RepoPart.DATA)
    assert (local_data_path / "restored_file.txt").exists()
    assert (local_data_path / "restored_file.txt").read_text() == "Restored content"
    from repoyard.cmds import delete_repo
    
    await delete_repo(config_path=config_path, repo_index_name=repo_index_name)
