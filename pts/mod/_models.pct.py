# ---
# jupyter:
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# # _models

# %%
#|default_exp _models

# %%
#|hide
from nblite import nbl_export, show_doc; nbl_export();

# %%
#|export
from pydantic import Field, model_validator
from pathlib import Path
import toml
from datetime import datetime, timezone
import random
from ulid import ULID
from enum import Enum
import repoyard.config
from repoyard import const
from repoyard.config import RepoGroupConfig, RepoTimestampFormat

# %% [markdown]
# # `RepoMeta`

# %%
#|export
class RepoPart(Enum):
    DATA = "data"
    META = "meta"
    CONF = "conf"

# %%
#|exporti
def _create_repo_subid(character_set: str, length: int) -> str:
    return "".join(random.choices(character_set, k=length))

# %%
#|export
def generate_unique_repo_id(
    config: repoyard.config.Config,
    existing_ids: set[str],
    max_attempts: int = 100,
) -> tuple[str, str]:
    """
    Generate a repo ID that doesn't collide with existing IDs.

    Args:
        config: Repoyard config (for timestamp format and subid settings)
        existing_ids: Set of existing repo IDs to check against
        max_attempts: Maximum generation attempts before raising error

    Returns:
        Tuple of (creation_timestamp, repo_subid)

    Raises:
        RuntimeError: If unable to generate unique ID after max_attempts
    """
    from repoyard.config import RepoTimestampFormat

    for _ in range(max_attempts):
        if config.repo_timestamp_format == RepoTimestampFormat.DATE_AND_TIME:
            creation_timestamp = datetime.now(timezone.utc).strftime(
                const.REPO_TIMESTAMP_FORMAT
            )
        elif config.repo_timestamp_format == RepoTimestampFormat.DATE_ONLY:
            creation_timestamp = datetime.now(timezone.utc).strftime(
                const.REPO_TIMESTAMP_FORMAT_DATE_ONLY
            )
        else:
            raise Exception(
                f"Invalid repo timestamp format: {config.repo_timestamp_format}"
            )

        repo_subid = _create_repo_subid(
            config.repo_subid_character_set, config.repo_subid_length
        )
        repo_id = f"{creation_timestamp}_{repo_subid}"

        if repo_id not in existing_ids:
            return creation_timestamp, repo_subid

    raise RuntimeError(
        f"Failed to generate unique repo ID after {max_attempts} attempts. "
        f"This should be extremely rare - please report this issue."
    )

# %%
#|export
class RepoMeta(const.StrictModel):
    creation_timestamp_utc: str
    repo_subid: str
    name: str
    storage_location: str
    creator_hostname: str
    groups: list[str]

    @classmethod
    def create(
        cls,
        config: repoyard.config.Config,
        name: str,
        storage_location_name: str,
        creator_hostname: str,
        groups: list[str],
        creation_timestamp_utc: datetime | None = None,
    ) -> "RepoMeta":
        if creation_timestamp_utc is None:
            if config.repo_timestamp_format == RepoTimestampFormat.DATE_AND_TIME:
                creation_timestamp_utc = datetime.now(timezone.utc).strftime(
                    const.REPO_TIMESTAMP_FORMAT
                )
            elif config.repo_timestamp_format == RepoTimestampFormat.DATE_ONLY:
                creation_timestamp_utc = datetime.now(timezone.utc).strftime(
                    const.REPO_TIMESTAMP_FORMAT_DATE_ONLY
                )
            else:
                raise Exception(
                    f"Invalid repo timestamp format: {config.repo_timestamp_format}"
                )
        else:
            if "_" in creation_timestamp_utc:
                creation_timestamp_utc = creation_timestamp_utc.strftime(
                    const.REPO_TIMESTAMP_FORMAT
                )
            else:
                creation_timestamp_utc = creation_timestamp_utc.strftime(
                    const.REPO_TIMESTAMP_FORMAT_DATE_ONLY
                )

        return RepoMeta(
            creation_timestamp_utc=creation_timestamp_utc,
            repo_subid=_create_repo_subid(
                config.repo_subid_character_set, config.repo_subid_length
            ),
            name=name,
            storage_location=storage_location_name,
            creator_hostname=creator_hostname,
            groups=groups,
        )

    @property
    def creation_timestamp_datetime(self) -> datetime:
        if "_" in self.creation_timestamp_utc:
            return datetime.strptime(
                self.creation_timestamp_utc, const.REPO_TIMESTAMP_FORMAT
            )
        else:
            return datetime.strptime(
                self.creation_timestamp_utc, const.REPO_TIMESTAMP_FORMAT_DATE_ONLY
            )

    @property
    def repo_id(self) -> str:
        return f"{self.creation_timestamp_utc}_{str(self.repo_subid)}"

    @property
    def index_name(self) -> str:
        return f"{self.repo_id}__{self.name}"

    @classmethod
    def parse_index_name(cls, index_name: str) -> tuple[str, str]:
        """Parse index_name into (repo_id, name)."""
        parts = index_name.split("__", 1)
        if len(parts) != 2:
            raise ValueError(f"Invalid index_name format: {index_name}")
        return parts[0], parts[1]

    @classmethod
    def extract_repo_id(cls, index_name: str) -> str:
        """Extract just the repo_id from an index_name."""
        return cls.parse_index_name(index_name)[0]

    def get_storage_location_config(
        self, config: repoyard.config.StorageConfig
    ) -> repoyard.config.StorageConfig:
        return config.storage_locations[self.storage_location]

    def get_remote_path(self, config: repoyard.config.Config) -> Path:
        return (
            config.storage_locations[self.storage_location].store_path
            / const.REMOTE_REPOS_REL_PATH
            / self.index_name
        )

    def get_local_path(self, config: repoyard.config.Config) -> Path:
        return config.local_store_path / self.storage_location / self.index_name

    def get_remote_part_path(
        self, config: repoyard.config.Config, repo_part: RepoPart
    ) -> Path:
        if repo_part == RepoPart.DATA:
            return self.get_remote_path(config) / const.REPO_DATA_REL_PATH
        elif repo_part == RepoPart.META:
            return self.get_remote_path(config) / const.REPO_METAFILE_REL_PATH
        elif repo_part == RepoPart.CONF:
            return self.get_remote_path(config) / const.REPO_CONF_REL_PATH
        else:
            raise ValueError(f"Invalid repo part: {repo_part}")

    def get_local_part_path(
        self, config: repoyard.config.Config, repo_part: RepoPart
    ) -> Path:
        if repo_part == RepoPart.DATA:
            return config.user_repos_path / self.index_name
        elif repo_part == RepoPart.META:
            return self.get_local_path(config) / const.REPO_METAFILE_REL_PATH
        elif repo_part == RepoPart.CONF:
            return self.get_local_path(config) / const.REPO_CONF_REL_PATH
        else:
            raise ValueError(f"Invalid repo part: {repo_part}")

    def get_remote_sync_record_path(
        self, config: repoyard.config.Config, repo_part: RepoPart
    ) -> Path:
        sl_conf = self.get_storage_location_config(config)
        return (
            sl_conf.store_path
            / const.SYNC_RECORDS_REL_PATH
            / self.index_name
            / f"{repo_part.value}.rec"
        )

    def get_local_sync_record_path(
        self, config: repoyard.config.Config, repo_part: RepoPart
    ) -> Path:
        return (
            config.repoyard_data_path
            / const.SYNC_RECORDS_REL_PATH
            / self.index_name
            / f"{repo_part.value}.rec"
        )

    def check_included(self, config: repoyard.config.Config) -> bool:
        included_repo_path = self.get_local_part_path(config, RepoPart.DATA)
        return included_repo_path.is_dir() and included_repo_path.exists()

    def save(self, config: repoyard.config.Config):
        save_path = self.get_local_part_path(config, RepoPart.META)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        model_dump = self.model_dump()
        del model_dump["creation_timestamp_utc"]
        del model_dump["repo_subid"]
        del model_dump["name"]
        # Atomic write: temp file + rename
        tmp_path = save_path.with_suffix(".tmp")
        tmp_path.write_text(toml.dumps(model_dump))
        tmp_path.rename(save_path)

    @classmethod
    def load(
        cls,
        config: repoyard.config.Config,
        storage_location_name: str,
        repo_index_name: str,
    ) -> "RepoMeta":
        repo_id, name = repo_index_name.split("__", 1)
        repo_id_parts = repo_id.split("_")
        if len(repo_id_parts) == 3:
            creation_timestamp = f"{repo_id_parts[0]}_{repo_id_parts[1]}"
        elif len(repo_id_parts) == 2:
            creation_timestamp = repo_id_parts[0]
        else:
            raise ValueError(f"Invalid repo id: {repo_id}")
        repo_subid = repo_id_parts[-1]

        repometa_path = (
            config.local_store_path
            / storage_location_name
            / repo_index_name
            / const.REPO_METAFILE_REL_PATH
        )
        if not repometa_path.exists():
            raise ValueError(f"Repo meta file {repometa_path} does not exist.")

        return RepoMeta(
            **{
                **toml.loads(repometa_path.read_text()),
                "creation_timestamp_utc": creation_timestamp,
                "repo_subid": repo_subid,
                "name": name,
                "storage_location": storage_location_name,
            }
        )

    @classmethod
    def validate_group_name(cls, group_name: str) -> None:
        """
        Allowed characters: alphanumeric + `_`, `-`, `/`
        """
        import re

        pattern = r"^[A-Za-z0-9_\-/]+$"
        if not isinstance(group_name, str) or not re.match(pattern, group_name):
            raise ValueError(
                f"Invalid group name '{group_name}'. "
                "Allowed characters: alphanumeric, '_', '-', '/'."
            )

    @model_validator(mode="after")
    def validate_repo_meta(self):
        if len(self.groups) != len(set(self.groups)):
            raise ValueError("Groups must be unique.")

        for group_name in self.groups:
            self.validate_group_name(group_name)

        # Test that the creation timestamp is valid
        try:
            self.creation_timestamp_datetime
        except ValueError:
            raise ValueError("Creation timestamp is not valid.")

        return self

# %%
from tests.integration.conftest import create_repoyards
from repoyard.config import get_config

sl_name, _, _, config_path, data_path = create_repoyards()
config = get_config(config_path)

repo_meta = RepoMeta.create(config, "my_repo", sl_name, "creator_hostname", [])
repo_meta.save(config)
_repo_meta = RepoMeta.load(config, sl_name, repo_meta.index_name)

assert repo_meta.model_dump_json() == _repo_meta.model_dump_json()

# %% [markdown]
# # `RepoyardMeta`

# %%
#|export
class RepoyardMeta(const.StrictModel):
    repo_metas: list[RepoMeta]

    @property
    def by_storage_location(self) -> dict[str, dict[str, RepoMeta]]:
        if not hasattr(self, "__by_storage_location"):
            storage_location_names = set(rm.storage_location for rm in self.repo_metas)
            self.__by_storage_location = {
                sl_name: {
                    repo_meta.index_name: repo_meta
                    for repo_meta in self.repo_metas
                    if repo_meta.storage_location == sl_name
                }
                for sl_name in storage_location_names
            }
        return self.__by_storage_location

    @property
    def by_id(self) -> dict[str, RepoMeta]:
        if not hasattr(self, "__by_id"):
            self.__by_id = {
                repo_meta.repo_id: repo_meta for repo_meta in self.repo_metas
            }
        return self.__by_id

    @property
    def by_repo_id(self) -> dict[str, RepoMeta]:
        """Alias for by_id for clarity."""
        return self.by_id

    @property
    def by_index_name(self) -> dict[str, RepoMeta]:
        if not hasattr(self, "__by_index_name"):
            self.__by_index_name = {
                repo_meta.index_name: repo_meta for repo_meta in self.repo_metas
            }
        return self.__by_index_name

# %%
#|export
def create_repoyard_meta(config: repoyard.config.Config) -> RepoyardMeta:
    """Create a dict of all repo metas. To be saved in `config.repoyard_meta_path`."""
    repo_metas = []
    for storage_location_name in config.storage_locations:
        local_storage_location_path = config.local_store_path / storage_location_name
        for repo_path in local_storage_location_path.glob("*"):
            if repo_path.is_file():
                continue
            repo_metas.append(
                RepoMeta.load(config, storage_location_name, repo_path.name)
            )
    return RepoyardMeta(repo_metas=repo_metas)

# %%
#|export
def refresh_repoyard_meta(
    config: repoyard.config.Config,
    _skip_lock: bool = False,
) -> RepoyardMeta:
    from repoyard._utils.locking import RepoyardLockManager
    from contextlib import nullcontext

    lock_manager = RepoyardLockManager(config.repoyard_data_path)
    lock_context = nullcontext() if _skip_lock else lock_manager.global_lock()

    with lock_context:
        repoyard_meta = create_repoyard_meta(config)
        # Atomic write: temp file + rename
        tmp_path = config.repoyard_meta_path.with_suffix(".tmp")
        tmp_path.write_text(repoyard_meta.model_dump_json())
        tmp_path.rename(config.repoyard_meta_path)
    return repoyard_meta

# %%
#|export
def get_repoyard_meta(
    config: repoyard.config.Config,
    force_create: bool = False,
) -> RepoyardMeta:
    if not config.repoyard_meta_path.exists() or force_create:
        refresh_repoyard_meta(config)
    return RepoyardMeta.model_validate_json(config.repoyard_meta_path.read_text())

# %%
#|export
def get_repo_group_configs(
    config: repoyard.config.Config,
    repo_metas: list[RepoMeta],
) -> dict[str, RepoGroupConfig]:
    repo_group_configs = config.repo_groups.copy()
    for repo_meta in repo_metas:
        for group_name in repo_meta.groups:
            if group_name not in repo_group_configs:
                repo_group_configs[group_name] = RepoGroupConfig()
    return repo_group_configs, config.virtual_repo_groups

# %%
#|export
def create_user_repo_group_symlinks(
    config: repoyard.config.Config,
):
    from collections import defaultdict
    from repoyard.config import RepoGroupTitleMode, VirtualRepoGroupConfig

    repo_metas = [
        repo_meta
        for repo_meta in get_repoyard_meta(config).repo_metas
        if repo_meta.check_included(config)
    ]
    repo_metas.sort(key=lambda x: x.creation_timestamp_datetime)
    groups, virtual_repo_groups = get_repo_group_configs(config, repo_metas)
    symlink_paths = []

    for vg in virtual_repo_groups:
        if vg in groups:
            print(f"Warning: Virtual repo group '{vg}' is also a regular repo group.")
    groups.update(virtual_repo_groups)

    def _get_symlink_title(repo_meta: RepoMeta, group_config: RepoGroupConfig) -> str:
        if group_config.repo_title_mode == RepoGroupTitleMode.INDEX_NAME:
            title = repo_meta.index_name
        elif group_config.repo_title_mode == RepoGroupTitleMode.DATETIME_AND_NAME:
            title = f"{repo_meta.creation_timestamp_utc}__{repo_meta.name}"
        elif group_config.repo_title_mode == RepoGroupTitleMode.NAME:
            title = repo_meta.name
        else:
            raise Exception(f"Invalid repo title mode: {group_config.repo_title_mode}")
        return title

    # Generate all symlink paths to create
    _symlinks = []
    for group_name, group_config in groups.items():
        title_counter = defaultdict(int)
        group_symlink_name = group_config.symlink_name or group_name
        for repo_meta in repo_metas:
            if not repo_meta.check_included(config):
                continue
            if isinstance(group_config, VirtualRepoGroupConfig):
                if not group_config.is_in_group(repo_meta.groups):
                    continue
            else:
                if group_name not in repo_meta.groups:
                    continue
            dest_path = repo_meta.get_local_part_path(config, RepoPart.DATA)
            title = _get_symlink_title(repo_meta, group_config)
            if title_counter[title] > 1:
                title = f"{title} (CONFLICT {title_counter[title]})"  # TODO this will break if the title contains a `(CONFLICT ...`
            title_counter[title] += 1
            symlink_path = config.user_repo_groups_path / group_symlink_name / title
            _symlinks.append((dest_path, symlink_path))

    # Remove all existing symlinks that are not in the _symlinks list
    _symlink_paths = [symlink_path for _, symlink_path in _symlinks]
    for path in config.user_repo_groups_path.glob("**/*"):
        if path in _symlink_paths:
            continue
        if path.is_symlink():
            path.unlink()

    # Now check for any remaining debris
    def _inspect_folder(path: Path) -> None:
        if path.is_symlink():
            return
        for p in path.iterdir():
            if p.is_dir():
                _inspect_folder(p)
            else:
                if p not in _symlink_paths:
                    raise Exception(
                        f"File '{p}' is in the user repo group path '{config.user_repo_groups_path}'."
                    )

    for path in config.user_repo_groups_path.glob("*"):
        if path.is_dir():
            _inspect_folder(path)
        else:
            raise Exception(
                f"'{path}' is in the user repo group path '{config.user_repo_groups_path}' but is not a directory!"
            )

    # Create the symlinks
    for dest_path, symlink_path in _symlinks:
        symlink_path.parent.mkdir(parents=True, exist_ok=True)
        if (
            symlink_path.exists() or symlink_path.is_symlink()
        ):  # is_symlink() catches broken symlinks
            if symlink_path.is_symlink():
                if symlink_path.resolve() != dest_path.resolve():
                    symlink_path.unlink()
                else:
                    continue  # The symlink already points to the correct destination so leave it as it is
            else:
                raise Exception(
                    f"'{symlink_path}' is in the user repo group path '{config.user_repo_groups_path}' but is not a symlink!"
                )
        symlink_path.symlink_to(dest_path, target_is_directory=True)

    # Remove all empty group folders that are not existing groups
    def _remove_empty_non_group_folders(path: Path) -> None:
        if path.is_symlink():
            return
        for p in path.iterdir():
            if p.is_dir():
                _remove_empty_non_group_folders(p)
        is_group_folder = (
            path.relative_to(config.user_repo_groups_path).as_posix() in groups
        )
        if not is_group_folder and len(list(path.iterdir())) == 0:
            path.rmdir()

    for path in config.user_repo_groups_path.glob("*"):
        _remove_empty_non_group_folders(path)

# %% [markdown]
# # `SyncRecord`

# %%
#|export
class SyncRecord(const.StrictModel):
    ulid: ULID = Field(default_factory=ULID)
    timestamp: datetime | None = (
        None  # Is set after validation. It's just to read it easier in printouts.
    )
    sync_complete: bool
    syncer_hostname: str

    @classmethod
    def create(cls, sync_complete: bool, syncer_hostname: str | None = None) -> None:
        from repoyard._utils import get_hostname

        return SyncRecord(
            sync_complete=sync_complete,
            syncer_hostname=syncer_hostname or get_hostname(),
        )

    async def rclone_save(
        self, rclone_config_path: str, dest: str, dest_path: str
    ) -> None:
        from repoyard._utils import rclone_copyto
        import tempfile

        temp_path = Path(tempfile.mkstemp(suffix=".json")[1])
        temp_path.write_text(self.model_dump_json())
        await rclone_copyto(
            rclone_config_path=rclone_config_path,
            source="",
            source_path=temp_path.as_posix(),
            dest=dest,
            dest_path=dest_path,
            dry_run=False,
        )

    @classmethod
    async def rclone_read(
        cls, rclone_config_path: str, source: str, sync_record_path: str
    ) -> str:
        from repoyard._utils import rclone_cat

        sync_record_exists, sync_record = await rclone_cat(
            rclone_config_path=rclone_config_path,
            source=source,
            source_path=sync_record_path,
        )

        if sync_record_exists:
            return SyncRecord.model_validate_json(sync_record)
        else:
            return None

    @model_validator(mode="after")
    def validate_timestamp(self):
        if self.timestamp is None:
            self.timestamp = self.ulid.datetime
        if self.timestamp != self.ulid.datetime:
            raise ValueError("`timestamp` should be set to the ULID's datetime.")
        return self

# %%
#|export
from typing import NamedTuple


class SyncCondition(Enum):
    SYNCED = "synced"
    SYNC_TO_REMOTE_INCOMPLETE = "sync_to_remote_incomplete"  # Push was interrupted, remote is corrupted
    SYNC_FROM_REMOTE_INCOMPLETE = "sync_from_remote_incomplete"  # Pull was interrupted, local is corrupted
    CONFLICT = "conflict"
    NEEDS_PUSH = "needs_push"
    NEEDS_PULL = "needs_pull"
    EXCLUDED = "excluded"
    ERROR = "error"
    TOMBSTONED = "tombstoned"  # Repo was deleted on remote


class SyncStatus(NamedTuple):
    sync_condition: SyncCondition
    local_path_exists: bool
    remote_path_exists: bool
    local_sync_record: SyncRecord
    remote_sync_record: SyncRecord
    is_dir: bool
    error_message: str | None = None

# %%
#|export
async def get_sync_status(
    rclone_config_path: str,
    local_path: str,
    local_sync_record_path: str,
    remote: str,
    remote_path: str,
    remote_sync_record_path: str,
) -> SyncStatus:
    from repoyard._utils import check_last_time_modified
    from repoyard._utils import rclone_path_exists

    local_path_exists, local_path_is_dir = await rclone_path_exists(
        rclone_config_path=rclone_config_path,
        source="",
        source_path=local_path,
    )
    local_path_is_empty = (
        True  # Default: treat as empty if doesn't exist or isn't a dir
    )
    if local_path_is_dir and local_path_exists:
        local_path_is_empty = len(list(local_path.iterdir())) == 0

    remote_path_exists, remote_path_is_dir = await rclone_path_exists(
        rclone_config_path=rclone_config_path,
        source=remote,
        source_path=remote_path,
    )

    if (local_path_exists and remote_path_exists) and (
        local_path_is_dir != remote_path_is_dir
    ):
        _local = "directory" if local_path_is_dir else "file"
        _remote = "directory" if remote_path_is_dir else "file"
        raise Exception(
            f"Local and remote paths are not both files or both directories. Local is {_local} and remote is {_remote}. Local path: '{local_path}', remote path: '{remote_path}'."
        )

    is_dir = local_path_is_dir or remote_path_is_dir

    local_sync_record = await SyncRecord.rclone_read(
        rclone_config_path=rclone_config_path,
        source="",
        sync_record_path=local_sync_record_path,
    )

    remote_sync_record = await SyncRecord.rclone_read(
        rclone_config_path=rclone_config_path,
        source=remote,
        sync_record_path=remote_sync_record_path,
    )

    local_sync_incomplete = (
        local_sync_record is not None and not local_sync_record.sync_complete
    )
    remote_sync_incomplete = (
        remote_sync_record is not None and not remote_sync_record.sync_complete
    )

    sync_records_match = (
        local_sync_record is not None and remote_sync_record is not None
    ) and (local_sync_record.ulid == remote_sync_record.ulid)

    sync_status = dict(
        local_path_exists=local_path_exists,
        remote_path_exists=remote_path_exists,
        local_sync_record=local_sync_record,
        remote_sync_record=remote_sync_record,
        is_dir=is_dir,
    )

    if remote_path_exists and remote_sync_record is None:
        sync_status["sync_condition"] = SyncCondition.ERROR
        sync_status["error_message"] = (
            f"Something wrong here. Remote path exists, but remote sync record does not exist. Local path: '{local_path}', remote path: '{remote_path}."
        )
        return SyncStatus(**sync_status)

    local_last_modified = check_last_time_modified(local_path)
    if local_last_modified is None and local_path_exists:
        if (not local_path_is_dir) or (local_path_is_dir and not local_path_is_empty):
            # Logic here: If the local path is a file, it should be able to be checked for last modification.
            # If the local path is a non-empty directory, it should also be able to be checked for last modification.
            sync_status["sync_condition"] = SyncCondition.ERROR
            sync_status["error_message"] = (
                f"Something wrong here. Local path exists and is not empty, but cannot be checked for last modification. Local path: '{local_path}', remote path: '{remote_path}."
            )
            return SyncStatus(**sync_status)

    if local_sync_incomplete and remote_sync_incomplete:
        if local_sync_record.ulid == remote_sync_record.ulid:
            # Same sync session - both sides marked by same operation
            # This is an interrupted PUSH from THIS machine (PUSH saves incomplete to both sides)
            sync_condition = SyncCondition.SYNC_TO_REMOTE_INCOMPLETE
        else:
            # Different ULIDs - inconsistent state, shouldn't happen in normal operation
            sync_status["error_message"] = (
                f"Inconsistent incomplete records (different ULIDs). "
                f"Local ULID: {local_sync_record.ulid}, Remote ULID: {remote_sync_record.ulid}"
            )
            sync_status["sync_condition"] = SyncCondition.ERROR
            return SyncStatus(**sync_status)
    elif remote_sync_incomplete:
        # Only remote is incomplete - push was interrupted (possibly from another machine)
        sync_condition = SyncCondition.SYNC_TO_REMOTE_INCOMPLETE
    elif local_sync_incomplete:
        # Only local is incomplete - pull was interrupted from THIS machine
        sync_condition = SyncCondition.SYNC_FROM_REMOTE_INCOMPLETE
    else:
        if sync_records_match:
            if (
                local_last_modified is not None
                and local_last_modified > local_sync_record.timestamp
            ):
                sync_condition = SyncCondition.NEEDS_PUSH
            else:
                sync_condition = SyncCondition.SYNCED
        else:
            if local_path_exists:
                if remote_path_exists:
                    if local_sync_record is None:
                        sync_status["sync_condition"] = SyncCondition.ERROR
                        sync_status["error_message"] = (
                            f"Something wrong here. Local sync record does not exist, but the local and remote path exists. Local path: '{local_path}', remote path: '{remote_path}."
                        )
                        return SyncStatus(**sync_status)
                    remote_sync_more_recent = (
                        remote_sync_record.ulid.datetime
                        > local_sync_record.ulid.datetime
                    )
                    if remote_sync_more_recent:
                        if (
                            local_last_modified is not None
                            and local_last_modified > local_sync_record.timestamp
                        ):
                            sync_condition = SyncCondition.CONFLICT
                        else:
                            sync_condition = SyncCondition.NEEDS_PULL
                    else:
                        sync_condition = SyncCondition.CONFLICT
                else:
                    if local_sync_record is not None:
                        sync_status["sync_condition"] = SyncCondition.ERROR
                        sync_status["error_message"] = (
                            f"Something wrong here. Local sync record exists, but remote path does not exist. Local path: '{local_path}', remote path: '{remote_path}."
                        )
                        return SyncStatus(**sync_status)
                    sync_condition = SyncCondition.NEEDS_PUSH
            else:
                if remote_path_exists:
                    sync_condition = SyncCondition.EXCLUDED
                else:
                    sync_condition = SyncCondition.SYNCED  # Synced by default, since neither local nor remote path exists. This will often be the case for `conf`, for example.

    sync_status["sync_condition"] = sync_condition
    return SyncStatus(**sync_status)
