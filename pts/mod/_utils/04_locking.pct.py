# ---
# jupyter:
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# # _utils.locking

# %%
#|default_exp _utils.locking

# %%
#|hide
from nblite import nbl_export, show_doc; nbl_export();

# %%
#|export
from pathlib import Path
from contextlib import contextmanager, asynccontextmanager
from filelock import FileLock, Timeout
import asyncio
from typing import Iterator

# %% [markdown]
# # Constants

# %%
#|export
GLOBAL_LOCK_TIMEOUT = 30  # seconds
REPO_SYNC_LOCK_TIMEOUT = 600  # 10 minutes

# %% [markdown]
# # Exceptions

# %%
#|export
class LockAcquisitionError(Exception):
    """Raised when a lock cannot be acquired within the timeout period."""

    def __init__(self, lock_type: str, lock_path: Path, timeout: float, message: str | None = None):
        self.lock_type = lock_type
        self.lock_path = lock_path
        self.timeout = timeout
        if message is None:
            message = (
                f"Could not acquire {lock_type} lock within {timeout}s. "
                f"Another repoyard operation may be in progress. "
                f"Lock file: {lock_path}"
            )
        super().__init__(message)

# %% [markdown]
# # `RepoyardLockManager`

# %%
#|export
class RepoyardLockManager:
    """
    Manages file-based locks for repoyard operations.

    Lock Directory Structure:
        ~/.repoyard/locks/
            global.lock                    # Protects repoyard_meta.json
            repos/{index_name}/
                sync.lock                  # Per-repo sync operations
    """

    def __init__(self, repoyard_data_path: Path):
        self.repoyard_data_path = Path(repoyard_data_path)
        self.locks_path = self.repoyard_data_path / "locks"
        self._active_locks: dict[Path, FileLock] = {}

    @property
    def global_lock_path(self) -> Path:
        return self.locks_path / "global.lock"

    def repo_sync_lock_path(self, index_name: str) -> Path:
        return self.locks_path / "repos" / index_name / "sync.lock"

    def _ensure_lock_dir(self, lock_path: Path) -> None:
        """Ensure the parent directory for a lock file exists."""
        lock_path.parent.mkdir(parents=True, exist_ok=True)

    @contextmanager
    def global_lock(self, timeout: float = GLOBAL_LOCK_TIMEOUT) -> Iterator[None]:
        """
        Context manager for acquiring the global lock.

        Use this when modifying repoyard_meta.json or performing operations
        that affect the global state.

        Args:
            timeout: Maximum time to wait for the lock in seconds.

        Raises:
            LockAcquisitionError: If the lock cannot be acquired within the timeout.
        """
        self._ensure_lock_dir(self.global_lock_path)
        lock = FileLock(self.global_lock_path, timeout=timeout)
        try:
            lock.acquire()
            yield
        except Timeout:
            raise LockAcquisitionError("global", self.global_lock_path, timeout)
        finally:
            if lock.is_locked:
                lock.release()

    @contextmanager
    def repo_sync_lock(
        self,
        index_name: str,
        timeout: float = REPO_SYNC_LOCK_TIMEOUT
    ) -> Iterator[None]:
        """
        Context manager for acquiring a per-repository sync lock.

        Use this for sync, include, exclude, and delete operations on a specific repo.

        Args:
            index_name: The repository index name.
            timeout: Maximum time to wait for the lock in seconds.

        Raises:
            LockAcquisitionError: If the lock cannot be acquired within the timeout.
        """
        lock_path = self.repo_sync_lock_path(index_name)
        self._ensure_lock_dir(lock_path)
        lock = FileLock(lock_path, timeout=timeout)
        try:
            lock.acquire()
            yield
        except Timeout:
            raise LockAcquisitionError(
                f"repo sync ({index_name})",
                lock_path,
                timeout,
                message=(
                    f"Could not acquire sync lock for repo '{index_name}' within {timeout}s. "
                    f"Another sync, include, exclude, or delete operation may be in progress on this repo."
                )
            )
        finally:
            if lock.is_locked:
                lock.release()

    @contextmanager
    def multiple_repo_sync_locks(
        self,
        index_names: list[str],
        timeout: float = REPO_SYNC_LOCK_TIMEOUT
    ) -> Iterator[None]:
        """
        Context manager for acquiring locks on multiple repositories.

        Acquires locks in alphabetical order to prevent deadlocks.

        Args:
            index_names: List of repository index names.
            timeout: Maximum time to wait for each lock in seconds.

        Raises:
            LockAcquisitionError: If any lock cannot be acquired within the timeout.
        """
        # Sort to prevent deadlocks
        sorted_names = sorted(set(index_names))
        acquired_locks: list[FileLock] = []

        try:
            for name in sorted_names:
                lock_path = self.repo_sync_lock_path(name)
                self._ensure_lock_dir(lock_path)
                lock = FileLock(lock_path, timeout=timeout)
                try:
                    lock.acquire()
                    acquired_locks.append(lock)
                except Timeout:
                    raise LockAcquisitionError(
                        f"repo sync ({name})",
                        lock_path,
                        timeout,
                        message=(
                            f"Could not acquire sync lock for repo '{name}' within {timeout}s. "
                            f"Another operation may be in progress on this repo."
                        )
                    )
            yield
        finally:
            # Release in reverse order
            for lock in reversed(acquired_locks):
                if lock.is_locked:
                    lock.release()

# %% [markdown]
# # Async Context Managers

# %%
#|export
@asynccontextmanager
async def async_global_lock(
    lock_manager: RepoyardLockManager,
    timeout: float = GLOBAL_LOCK_TIMEOUT
):
    """
    Async context manager for acquiring the global lock.

    This wraps the synchronous lock in an executor to avoid blocking the event loop.
    """
    loop = asyncio.get_event_loop()

    lock_path = lock_manager.global_lock_path
    lock_manager._ensure_lock_dir(lock_path)
    lock = FileLock(lock_path, timeout=timeout)

    try:
        await loop.run_in_executor(None, lock.acquire)
        yield
    except Timeout:
        raise LockAcquisitionError("global", lock_path, timeout)
    finally:
        if lock.is_locked:
            await loop.run_in_executor(None, lock.release)


@asynccontextmanager
async def async_repo_sync_lock(
    lock_manager: RepoyardLockManager,
    index_name: str,
    timeout: float = REPO_SYNC_LOCK_TIMEOUT
):
    """
    Async context manager for acquiring a per-repository sync lock.

    This wraps the synchronous lock in an executor to avoid blocking the event loop.
    """
    loop = asyncio.get_event_loop()

    lock_path = lock_manager.repo_sync_lock_path(index_name)
    lock_manager._ensure_lock_dir(lock_path)
    lock = FileLock(lock_path, timeout=timeout)

    try:
        await loop.run_in_executor(None, lock.acquire)
        yield
    except Timeout:
        raise LockAcquisitionError(
            f"repo sync ({index_name})",
            lock_path,
            timeout,
            message=(
                f"Could not acquire sync lock for repo '{index_name}' within {timeout}s. "
                f"Another sync, include, exclude, or delete operation may be in progress on this repo."
            )
        )
    finally:
        if lock.is_locked:
            await loop.run_in_executor(None, lock.release)

# %% [markdown]
# # Utility Functions

# %%
#|export
def cleanup_stale_locks(
    repoyard_data_path: Path,
    max_age_hours: float = 24
) -> list[Path]:
    """
    Remove lock files that are older than the specified age AND not currently held.

    This safely cleans up locks from crashed processes without affecting
    long-running operations that legitimately hold locks.

    Args:
        repoyard_data_path: Path to the repoyard data directory.
        max_age_hours: Maximum age of lock files in hours before they're considered stale.

    Returns:
        List of paths to removed lock files.
    """
    import time

    locks_path = Path(repoyard_data_path) / "locks"
    if not locks_path.exists():
        return []

    removed = []
    max_age_seconds = max_age_hours * 3600
    current_time = time.time()

    for lock_file in locks_path.rglob("*.lock"):
        try:
            file_age = current_time - lock_file.stat().st_mtime
            if file_age > max_age_seconds:
                # Try to acquire the lock with zero timeout to check if it's held
                test_lock = FileLock(lock_file, timeout=0)
                try:
                    test_lock.acquire()
                    # We got the lock, so no one else is holding it - safe to delete
                    test_lock.release()
                    lock_file.unlink()
                    removed.append(lock_file)
                except Timeout:
                    # Lock is currently held by another process - don't delete
                    pass
        except (OSError, FileNotFoundError):
            # File may have been removed by another process
            pass

    return removed


def auto_cleanup_stale_locks(
    repoyard_data_path: Path,
    max_age_hours: float = 1.0,
    verbose: bool = False
) -> list[Path]:
    """
    Automatically clean up stale locks on startup.

    This is less aggressive than cleanup_stale_locks - it only removes
    locks older than 1 hour by default (vs 24 hours).

    Args:
        repoyard_data_path: Path to the repoyard data directory.
        max_age_hours: Maximum age of lock files in hours before they're considered stale.
        verbose: If True, print a message when locks are cleaned up.

    Returns:
        List of paths to removed lock files.
    """
    removed = cleanup_stale_locks(repoyard_data_path, max_age_hours)
    if removed and verbose:
        print(f"Cleaned up {len(removed)} stale lock file(s):")
        for path in removed:
            print(f"  - {path}")
    return removed

# %% [markdown]
# # Cancellation-safe async lock acquisition

# %%
#|export
LOCK_POLL_INTERVAL = 0.1  # seconds between lock acquisition attempts


async def acquire_lock_async(
    lock: FileLock,
    lock_type: str,
    lock_path: Path,
    timeout: float
) -> None:
    """
    Acquire a lock asynchronously using polling.

    This approach is cancellation-safe - if the coroutine is cancelled,
    no lock will be left in an inconsistent state.
    """
    import time
    deadline = time.time() + timeout

    while True:
        try:
            # Try to acquire with zero timeout (non-blocking)
            lock.acquire(timeout=0)
            return  # Successfully acquired
        except Timeout:
            # Check if we've exceeded the deadline
            if time.time() >= deadline:
                raise LockAcquisitionError(lock_type, lock_path, timeout)
            # Yield to the event loop - this is where cancellation can happen safely
            await asyncio.sleep(LOCK_POLL_INTERVAL)

# %% [markdown]
# # Tests

# %%
from tests.utils import create_repoyards

sl_name, _, config, config_path, data_path = create_repoyards()

# %%
# Test basic global lock
lock_manager = RepoyardLockManager(data_path)

with lock_manager.global_lock():
    assert lock_manager.global_lock_path.exists()
    print("Global lock acquired")

print("Global lock released")

# %%
# Test repo sync lock
with lock_manager.repo_sync_lock("test_repo__index"):
    assert lock_manager.repo_sync_lock_path("test_repo__index").exists()
    print("Repo sync lock acquired")

print("Repo sync lock released")

# %%
# Test multiple repo sync locks (should acquire in alphabetical order)
with lock_manager.multiple_repo_sync_locks(["z_repo", "a_repo", "m_repo"]):
    print("Multiple locks acquired")

print("Multiple locks released")

# %%
# Test async locks
async def test_async_locks():
    async with async_global_lock(lock_manager):
        print("Async global lock acquired")
    print("Async global lock released")

    async with async_repo_sync_lock(lock_manager, "async_test_repo"):
        print("Async repo sync lock acquired")
    print("Async repo sync lock released")

await test_async_locks()
