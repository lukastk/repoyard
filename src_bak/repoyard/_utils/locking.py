# AUTOGENERATED! DO NOT EDIT! File to edit: pts/mod/_utils/04_locking.pct.py

__all__ = [
    "GLOBAL_LOCK_TIMEOUT",
    "LockAcquisitionError",
    "REPO_SYNC_LOCK_TIMEOUT",
    "RepoyardLockManager",
    "async_global_lock",
    "async_repo_sync_lock",
    "cleanup_stale_locks",
]

# %% pts/mod/_utils/04_locking.pct.py 3
import asyncio
from collections.abc import Iterator
from contextlib import asynccontextmanager, contextmanager
from pathlib import Path

from filelock import FileLock, Timeout

# %% pts/mod/_utils/04_locking.pct.py 5
GLOBAL_LOCK_TIMEOUT = 30  # seconds
REPO_SYNC_LOCK_TIMEOUT = 600  # 10 minutes


# %% pts/mod/_utils/04_locking.pct.py 7
class LockAcquisitionError(Exception):
    """Raised when a lock cannot be acquired within the timeout period."""

    def __init__(self, lock_type: str, lock_path: Path, timeout: float, message: str | None = None):
        self.lock_type = lock_type
        self.lock_path = lock_path
        self.timeout = timeout
        if message is None:
            message = (
                f"Could not acquire {lock_type} lock within {timeout}s. "
                f"Another repoyard operation may be in progress. "
                f"Lock file: {lock_path}"
            )
        super().__init__(message)


# %% pts/mod/_utils/04_locking.pct.py 9
class RepoyardLockManager:
    """
    Manages file-based locks for repoyard operations.

    Lock Directory Structure:
        ~/.repoyard/locks/
            global.lock                    # Protects repoyard_meta.json
            repos/{index_name}/
                sync.lock                  # Per-repo sync operations
    """

    def __init__(self, repoyard_data_path: Path):
        self.repoyard_data_path = Path(repoyard_data_path)
        self.locks_path = self.repoyard_data_path / "locks"
        self._active_locks: dict[Path, FileLock] = {}

    @property
    def global_lock_path(self) -> Path:
        return self.locks_path / "global.lock"

    def repo_sync_lock_path(self, index_name: str) -> Path:
        return self.locks_path / "repos" / index_name / "sync.lock"

    def _ensure_lock_dir(self, lock_path: Path) -> None:
        """Ensure the parent directory for a lock file exists."""
        lock_path.parent.mkdir(parents=True, exist_ok=True)

    @contextmanager
    def global_lock(self, timeout: float = GLOBAL_LOCK_TIMEOUT) -> Iterator[None]:
        """
        Context manager for acquiring the global lock.

        Use this when modifying repoyard_meta.json or performing operations
        that affect the global state.

        Args:
            timeout: Maximum time to wait for the lock in seconds.

        Raises:
            LockAcquisitionError: If the lock cannot be acquired within the timeout.
        """
        self._ensure_lock_dir(self.global_lock_path)
        lock = FileLock(self.global_lock_path, timeout=timeout)
        try:
            lock.acquire()
            yield
        except Timeout:
            raise LockAcquisitionError("global", self.global_lock_path, timeout)
        finally:
            if lock.is_locked:
                lock.release()

    @contextmanager
    def repo_sync_lock(self, index_name: str, timeout: float = REPO_SYNC_LOCK_TIMEOUT) -> Iterator[None]:
        """
        Context manager for acquiring a per-repository sync lock.

        Use this for sync, include, exclude, and delete operations on a specific repo.

        Args:
            index_name: The repository index name.
            timeout: Maximum time to wait for the lock in seconds.

        Raises:
            LockAcquisitionError: If the lock cannot be acquired within the timeout.
        """
        lock_path = self.repo_sync_lock_path(index_name)
        self._ensure_lock_dir(lock_path)
        lock = FileLock(lock_path, timeout=timeout)
        try:
            lock.acquire()
            yield
        except Timeout:
            raise LockAcquisitionError(
                f"repo sync ({index_name})",
                lock_path,
                timeout,
                message=(
                    f"Could not acquire sync lock for repo '{index_name}' within {timeout}s. "
                    f"Another sync, include, exclude, or delete operation may be in progress on this repo."
                ),
            )
        finally:
            if lock.is_locked:
                lock.release()

    @contextmanager
    def multiple_repo_sync_locks(
        self, index_names: list[str], timeout: float = REPO_SYNC_LOCK_TIMEOUT
    ) -> Iterator[None]:
        """
        Context manager for acquiring locks on multiple repositories.

        Acquires locks in alphabetical order to prevent deadlocks.

        Args:
            index_names: List of repository index names.
            timeout: Maximum time to wait for each lock in seconds.

        Raises:
            LockAcquisitionError: If any lock cannot be acquired within the timeout.
        """
        # Sort to prevent deadlocks
        sorted_names = sorted(set(index_names))
        acquired_locks: list[FileLock] = []

        try:
            for name in sorted_names:
                lock_path = self.repo_sync_lock_path(name)
                self._ensure_lock_dir(lock_path)
                lock = FileLock(lock_path, timeout=timeout)
                try:
                    lock.acquire()
                    acquired_locks.append(lock)
                except Timeout:
                    raise LockAcquisitionError(
                        f"repo sync ({name})",
                        lock_path,
                        timeout,
                        message=(
                            f"Could not acquire sync lock for repo '{name}' within {timeout}s. "
                            f"Another operation may be in progress on this repo."
                        ),
                    )
            yield
        finally:
            # Release in reverse order
            for lock in reversed(acquired_locks):
                if lock.is_locked:
                    lock.release()


# %% pts/mod/_utils/04_locking.pct.py 11
@asynccontextmanager
async def async_global_lock(lock_manager: RepoyardLockManager, timeout: float = GLOBAL_LOCK_TIMEOUT):
    """
    Async context manager for acquiring the global lock.

    This wraps the synchronous lock in an executor to avoid blocking the event loop.
    """
    loop = asyncio.get_event_loop()

    lock_path = lock_manager.global_lock_path
    lock_manager._ensure_lock_dir(lock_path)
    lock = FileLock(lock_path, timeout=timeout)

    try:
        await loop.run_in_executor(None, lock.acquire)
        yield
    except Timeout:
        raise LockAcquisitionError("global", lock_path, timeout)
    finally:
        if lock.is_locked:
            await loop.run_in_executor(None, lock.release)


@asynccontextmanager
async def async_repo_sync_lock(
    lock_manager: RepoyardLockManager, index_name: str, timeout: float = REPO_SYNC_LOCK_TIMEOUT
):
    """
    Async context manager for acquiring a per-repository sync lock.

    This wraps the synchronous lock in an executor to avoid blocking the event loop.
    """
    loop = asyncio.get_event_loop()

    lock_path = lock_manager.repo_sync_lock_path(index_name)
    lock_manager._ensure_lock_dir(lock_path)
    lock = FileLock(lock_path, timeout=timeout)

    try:
        await loop.run_in_executor(None, lock.acquire)
        yield
    except Timeout:
        raise LockAcquisitionError(
            f"repo sync ({index_name})",
            lock_path,
            timeout,
            message=(
                f"Could not acquire sync lock for repo '{index_name}' within {timeout}s. "
                f"Another sync, include, exclude, or delete operation may be in progress on this repo."
            ),
        )
    finally:
        if lock.is_locked:
            await loop.run_in_executor(None, lock.release)


# %% pts/mod/_utils/04_locking.pct.py 13
def cleanup_stale_locks(repoyard_data_path: Path, max_age_hours: float = 24) -> list[Path]:
    """
    Remove lock files that are older than the specified age.

    This can be used to clean up locks from crashed processes.

    Args:
        repoyard_data_path: Path to the repoyard data directory.
        max_age_hours: Maximum age of lock files in hours before they're considered stale.

    Returns:
        List of paths to removed lock files.
    """
    import time

    locks_path = Path(repoyard_data_path) / "locks"
    if not locks_path.exists():
        return []

    removed = []
    max_age_seconds = max_age_hours * 3600
    current_time = time.time()

    for lock_file in locks_path.rglob("*.lock"):
        try:
            file_age = current_time - lock_file.stat().st_mtime
            if file_age > max_age_seconds:
                lock_file.unlink()
                removed.append(lock_file)
        except (OSError, FileNotFoundError):
            # File may have been removed by another process
            pass

    return removed
